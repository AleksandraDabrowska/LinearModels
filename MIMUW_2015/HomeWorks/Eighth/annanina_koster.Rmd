---
title: "Homework 8"
author: "Annanina Koster"
date: "25 november 2015"
output: html_document
---

Home work for 26 XI 2015
For the Drosophila study check what will happen with coefficients if:

a) all variables are included into a model and standard MLE estimates are calculated, b) ridge regression is applied, c) lasso regression is applied.

For points b) and c) present how model coefficient behaves as a function of penalty/parameter.

- - -

First, I will load the dataset and estimate the coefficients of a linear model including all variables with MLE.

```{r, warning=FALSE, message=FALSE}

library(PBImisc)
attach(Drosophila)
head(bs)

# Standard MLE Regression:

model1 <- lm(pc1 ~ . , data=bs)
coef1 <- model1$coefficients
coef1
summary(coef1)
which(coef1 > 0.001)

```

As can be seen from the summary, most coefficients are approximately 0. Only 4 are larger than 0.001.

- - -

Then, I will apply the Ridge regression to estimate the coefficients of the same linear model. The chosen lamda is 1. It cannot be determined using a plot, since there are 45 variables. The coefficients are shown. Moreover, a plot is provided that shows how the coefficients depend on the chosen lamda.

```{r, warning=FALSE, message=FALSE}

# Ridge Regression:

model2 <- lm.ridge(pc1 ~ ., data=bs, lambda=1)
coef2 <- coef(model2)
coef2
summary(coef2)
which(coef2>0.001)

lambda1 <- seq(-10,100,1)
model3 <- lm.ridge(pc1 ~. , data=bs, lambda=lambda1)
plot(model3)


```

For the ridge regression also most coefficients are around 0 (for lambda = 1). This can be seen in the summary and plot. It can also be seen that lambda=0, would give unreliable results and that for increasing lambda the values of the coefficients all converge to zero eventually. Moreover, lambda should not be chosen negatively. Furthermore, the same 4 variables have estimated coefficients larger than 0.001 as before.

- - -

Afterwards, the same model is estimated, but now using LASSO regression. The estimated coefficients are depicted. Also for this case the plots are not informative because there are too many variables being considered.

```{r, warning=FALSE, message=FALSE}
# LASSO Regression:

library(lasso2)
lambda2 <- seq(1,100,1)
coef3 <- matrix(0,100,46)
for(i in 1:100){
  coef3[i,] <- l1ce(pc1 ~ . , data=bs, bound=lambda2[i], absolute.t=TRUE)$coef
}
coef3[10,]
which(coef3[10,]>0.001)
summary(coef3[10,])
summary(coef3[,10])
summary(coef3[,25])

```

From the first summary it can also be seen that most coefficients are estimated to be around 0 (for lambda=10). Moreover, from all of the estimated coefficients and the second and third summary it can be seen that the esimated coefficients are the same for all lambdas. Also, once again the same variables have estimated coefficients larger than 0.001.

- - -

Conclusion:
All three models estimate most coefficients around 0, except for a few (these are the same for all models). Moreover, the lambda does not matter for Lasso regression, whereas it does for ridge regression.