---
title: "Homework 8"
author: "Ma³gorzata Pujszo"
date: "Thursday, November 19, 2015"
output: html_document
---

In the dataset Drosophila, we would like to explain pc1 by genotypes. I got the coefficients of the genotypes using 3 different methods:

  a. calculating standard MLE estimators
  
  b. using ridge regression
  
  c. using LASSO regression
  
###a. MLE estimators 

```{r, warning=FALSE, message=FALSE}
library(PBImisc)
attach(Drosophila)
model <- lm(pc1~., data=bs[,1:42])
model$coefficients
max(abs(model$coefficients))
min(abs(model$coefficients))
```

As we can see all the coefficients are pretty close to 0. 

###b. Ridge regression estimators

```{r, warning=FALSE, message=FALSE}
lambdas <- 10^((-1):6)
coefs <- sapply(lambdas, function(lambda) {
  lm.ridge(pc1~., data=bs[,1:42], lambda=lambda)$coef
})
df2 <- data.frame(t(coefs), lambdas)
library(ggplot2)
library(reshape2)
mdf <- melt(df2, id="lambdas")
ggplot(mdf, aes(x=lambdas,y=value,color=variable)) +
  geom_line(size=1) + ylab("coefficients") + scale_x_log10() + theme(legend.position="none")
```

For all lambas coefficients are quite small (of order 10^(-3) or smaller). When lambda is bigger than 10^5, all coefficients are very close to 0, lambda equal to 0 obviously gives the same results as MLE estimators. If lambda is in (10^3, 10^5) many coefficients become closer to 0. For lambda in this interval we may choose smaller model which explains pc1 by not taking into acount variables which are almost 0. Here is an example when lambda=5*10^3. Minimal and maximal distance from 0 are calculated. 

```{r, warning=FALSE, message=FALSE}
model.ridge <- lm.ridge(pc1~., data=bs[,1:42], lambda=5*(10^3))
model.ridge$coef
min(abs(model.ridge$coef))
max(abs(model.ridge$coef))
```

In comparison with previous model (MLE estimator) the furthest value from 0 is of order 10^(-4), not 10^(-2).

###c. LASSO regression estimators

Now I performed LASSO regression for a set of bounds.

```{r, warning=FALSE, message=FALSE}
library(lasso2)
lambdas <- 10^seq(-6,1,0.1)
coefs <- sapply(lambdas, function(lambda) {
  l1ce(pc1~., data=bs[,1:42], bound=lambda, absolute.t=TRUE)$coef
})
df2 <- data.frame(t(coefs), lambdas)[,-1]
mdf <- melt(df2, id="lambdas")
ggplot(mdf, aes(x=lambdas,y=value,color=variable)) +
  geom_line(size=1) + ylab("coefficients") + scale_x_log10() + theme(legend.position="none")
```

From the plot, we can observe that for lambda equal to about 5*10^(-2) and less, some coefficients become 0. When lambda reaches about 10^(-5), all coefficients are 0. Here is an example of bound=10^(-2).

```{r, warning=FALSE, message=FALSE}
(coeffs <- l1ce(pc1~., data=bs[,1:42], bound=10^(-2), absolute.t=TRUE)$coef[-1])
length(coeffs[coeffs!=0])
```

For such lambda we have 22 coefficients not equal to 0. So we can choose smaller model.

###Conclusions

In all three approaches the absolute values of coefficients are pretty small. When we consider ridge regression as a function of penalty, coefficients' values are becoming closer to 0 (but not 0) with increasing lambdas. In the case of LASSO regression considering decreasing bounds, more and more coefficients become entirely 0.