---
title: "Praca domowa 8"
author: "Piotr Obarski"
date: "Modele liniowe i mieszane"
output: 
  html_document:
  toc: TRUE
---

For the Drosophila study check what will happen with coefficients if:

a) all variables are included into a model and standard MLE estimates are calculated, b) ridge regression is applied, c) lasso regression is applied.

For points b) and c) present how model coefficient behaves as a function of penalty/parameter.

# Solution
Fristly load dataset and do normal linear regression with the function lm.

```{r}
library(ggplot2)
library(PBImisc)
attach(Drosophila)

lm(pc1~., data=bs)
coef1<-lm(pc1~., data=bs)$coef
```

Now let's try ridge regression. I will draw a plot only for a few coefficients, but we know that coefficients, which are close to each other, are correlated, so I chose them in a way to have coefficients which are not correlated.

```{r}
lambdas <- 2^((-20):20)
coefs <- sapply(lambdas, function(lambda) {
  lm.ridge(pc1~., data=bs, lambda=lambda)$coef
})
df2 <- data.frame(t(coefs), lambdas)
ggplot(df2, aes(lambdas, ewg)) +
  geom_line(color="green3") + ylab("coefficients") +
  geom_line(aes(lambdas, run)) + scale_x_log10() +
  geom_line(aes(lambdas, gl), color="coral2") +
  geom_line(aes(lambdas, gpdh), color="deeppink2") +
  geom_line(aes(lambdas, ddc), color="orange3") +
  geom_line(aes(lambdas, sli), color="yellow3") +
  geom_line(aes(lambdas, ninaE), color="darkorchid3") +
  geom_line(aes(lambdas, ve), color="aquamarine1") +
  geom_line(aes(lambdas, dbi), color="lightgoldenrod2") +
  geom_line(aes(lambdas, rdg), color="tan") +
  geom_line(aes(lambdas, ninaE), color="azure2") +
  geom_line(aes(lambdas, jan), color="blue4") +
  geom_line(aes(lambdas, rdg), color="blueviolet") +
  geom_line(aes(lambdas, area), color="brown2") +
  geom_line(aes(lambdas, tibia), color="darkcyan")
```

We see that coefficient area differs very much from others, so let's remove it from the plot.

```{r}

ggplot(df2, aes(lambdas, ewg)) +
  geom_line(color="green3") + ylab("coefficients") +
  geom_line(aes(lambdas, run)) + scale_x_log10() +
  geom_line(aes(lambdas, gl), color="coral2") +
  geom_line(aes(lambdas, gpdh), color="deeppink2") +
  geom_line(aes(lambdas, ddc), color="orange3") +
  geom_line(aes(lambdas, sli), color="yellow3") +
  geom_line(aes(lambdas, ninaE), color="darkorchid3") +
  geom_line(aes(lambdas, ve), color="aquamarine1") +
  geom_line(aes(lambdas, dbi), color="lightgoldenrod2") +
  geom_line(aes(lambdas, rdg), color="tan") +
  geom_line(aes(lambdas, ninaE), color="azure2") +
  geom_line(aes(lambdas, jan), color="blue4") +
  geom_line(aes(lambdas, rdg), color="blueviolet") +
  geom_line(aes(lambdas, tibia), color="darkcyan")
```

From the plot we see that it can be worthwhile to do ridge regression for lambdas equal to 10, 100, and 1000000.
```{r}
lm.ridge(pc1~., data=bs, lambda=10)
lm.ridge(pc1~., data=bs, lambda=100)
lm.ridge(pc1~., data=bs, lambda=1000000)

coef5<-lm.ridge(pc1~., data=bs, lambda=10)$coef
coef2<-lm.ridge(pc1~., data=bs, lambda=100)$coef
coef3<-lm.ridge(pc1~., data=bs, lambda=1000000)$coef
```

Let's compute the distance between vectors with coefficients from normal regression and ridge regression in metric max.
```{r}
max(abs(coef1[2:46]-coef5))
max(abs(coef1[2:46]-coef2))
max(abs(coef1[2:46]-coef3))
```

Now we do the lasso regression.
```{r, warning=FALSE}
library(lasso2)


lambdas <- 10^seq(-5,3,0.1)
coefs <- sapply(lambdas, function(lambda) {
  l1ce(pc1~., data=bs, bound=lambda, absolute.t=TRUE)$coef
})
df2 <- data.frame(t(coefs), lambdas)

ggplot(df2, aes(lambdas, ewg)) +
  geom_line(color="green3") + ylab("coefficients") +
  geom_line(aes(lambdas, run)) + scale_x_log10() +
  geom_line(aes(lambdas, gl), color="coral2") +
  geom_line(aes(lambdas, gpdh), color="deeppink2") +
  geom_line(aes(lambdas, ddc), color="orange3") +
  geom_line(aes(lambdas, sli), color="yellow3") +
  geom_line(aes(lambdas, ninaE), color="darkorchid3") +
  geom_line(aes(lambdas, ve), color="aquamarine1") +
  geom_line(aes(lambdas, dbi), color="lightgoldenrod2") +
  geom_line(aes(lambdas, rdg), color="tan") +
  geom_line(aes(lambdas, ninaE), color="azure2") +
  geom_line(aes(lambdas, jan), color="blue4") +
  geom_line(aes(lambdas, rdg), color="blueviolet") +
  geom_line(aes(lambdas, area), color="brown2") +
  geom_line(aes(lambdas, tibia), color="darkcyan")
```

On the plot we see that again area is the coefficient which stands out the most.

See how the plot looks without this.
```{r}
ggplot(df2, aes(lambdas, ewg)) +
  geom_line(color="green3") + ylab("coefficients") +
  geom_line(aes(lambdas, run)) + scale_x_log10() +
  geom_line(aes(lambdas, gl), color="coral2") +
  geom_line(aes(lambdas, gpdh), color="deeppink2") +
  geom_line(aes(lambdas, ddc), color="orange3") +
  geom_line(aes(lambdas, sli), color="yellow3") +
  geom_line(aes(lambdas, ninaE), color="darkorchid3") +
  geom_line(aes(lambdas, ve), color="aquamarine1") +
  geom_line(aes(lambdas, dbi), color="lightgoldenrod2") +
  geom_line(aes(lambdas, rdg), color="tan") +
  geom_line(aes(lambdas, ninaE), color="azure2") +
  geom_line(aes(lambdas, jan), color="blue4") +
  geom_line(aes(lambdas, rdg), color="blueviolet") +
  geom_line(aes(lambdas, tibia), color="darkcyan")
```

Now tibia coefficient stands out the most, so remove it from the graph.
```{r}
ggplot(df2, aes(lambdas, ewg)) +
  geom_line(color="green3") + ylab("coefficients") +
  geom_line(aes(lambdas, run)) + scale_x_log10() +
  geom_line(aes(lambdas, gl), color="coral2") +
  geom_line(aes(lambdas, gpdh), color="deeppink2") +
  geom_line(aes(lambdas, ddc), color="orange3") +
  geom_line(aes(lambdas, sli), color="yellow3") +
  geom_line(aes(lambdas, ninaE), color="darkorchid3") +
  geom_line(aes(lambdas, ve), color="aquamarine1") +
  geom_line(aes(lambdas, dbi), color="lightgoldenrod2") +
  geom_line(aes(lambdas, rdg), color="tan") +
  geom_line(aes(lambdas, ninaE), color="azure2") +
  geom_line(aes(lambdas, jan), color="blue4") +
  geom_line(aes(lambdas, rdg), color="blueviolet")
```

It is very difficult to set a bound in lasso, beacuse all coefficients start to rise almost at the same moment, and reach their final value also almost at the same time. 

```{r}
l1ce(pc1~., data=bs, bound=.01, absolute.t=TRUE)

coef4<-l1ce(pc1~., data=bs, bound=.01, absolute.t=TRUE)$coef

max(abs(coef1-coef4))
max(abs(coef2-coef4[2:46]))
max(abs(coef3-coef4[2:46]))
max(abs(coef5-coef4[2:46]))
```

# Conclusions

In the max metric lasso model and model which uses MLE are very similar, in fact almost the same. Model made with the ridge regerssion is different from others. Also interesting thing happened in ridge regression. Some coefficients changed their sign. It may be caused by the fact that other coefficients increased in that "time", so to balance it, others had to decrease and they decreased so much that they changed their sign.
