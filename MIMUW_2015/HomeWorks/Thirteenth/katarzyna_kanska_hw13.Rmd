---
title: "Homework 13"
author: "Katarzyna Kanska"
date: "21.01.2016"
output: 
  html_document:
    toc: TRUE
---

## Goal

We will work with data about previous homeworks.

For each homework let's count the number of lines in the Rmd file. Then take into account two other variables: the id (number / week) of the homework and the person that is doing the homework.

Create a model that explains the number of lines of homework. What is the relation between number of lines of the homework and the id / week of the homework?

Should the author/submitter be modeled as a fixed or random effect?

## The dataset

Fistly, we cannot just load the dataset. We have to gather this data (for example using some .sh script).

Then we can summarize and plot this data.
```{r, warning=FALSE, message=FALSE}
results <- read.csv("~/results.csv")
summary(results)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5, fig.width=7}
library(ggplot2)

ggplot(results, aes(x=homework, y=lines)) +
  geom_point(size=2) +
  stat_smooth(aes(colour="linear"), method = "lm", se=FALSE, formula = y ~ x) +
  stat_smooth(aes(colour="2nd order"), method = "lm", se=FALSE, formula = y ~ x + I(x^2)) +
  stat_smooth(aes(colour="3rd order"), method = "lm", se=FALSE, formula = y ~ x + I(x^2) + I(x^3)) +
  scale_colour_discrete("") +
  ggtitle("Polynomial fit") + 
  theme(plot.title = element_text(lineheight=.8, face="bold"))
```

There is no linear relation between `lines` and `homework`. But it is probably worth considering higher order polynomial.

Now let us examine the effect of each student.

```{r, warning=FALSE, message=FALSE}
library(lattice)
# panel for lattice package
panel.with.square <- function(...){
  nx = list(...)$x
  ny = list(...)$y
  cc = lm(ny~nx+I(nx^2))$coef
  panel.xyplot(...)
  panel.curve(x^2*cc[3]+x*cc[2]+cc[1], from=min(nx), to=max(nx), col="red")
}
# the plot
xyplot(lines ~ homework | student, results, type = c("g","p","r"),
  xlab = "Homework", ylab = "Number of lines", pch=19,
  panel = panel.with.square)
```

We can see that the effect of `student` should be included. But we also see that not everyone did every homework. From external source of knowledge we can discover that a few people gave up the course. It is not clear if we should include these people in this study during the semester. But let us keep them.

## The linear model

Here we consider two models with `homework` treated as fixed effect (quantitative variable) and `student` as random effect. One of them additionally takes into account the possibility that each student may react differently.

```{r, warning=FALSE, message=FALSE}
library(lme4)
model1 <- lmer(lines ~ homework + (1|student), data=results)
model2 <- lmer(lines ~ homework + (homework|student), data=results)
```

Now we compare these models.

```{r, warning=FALSE, message=FALSE}
criteria <- matrix(nrow=2, ncol=2)
criteria[1,] <- c(AIC(model1),BIC(model1))
criteria[2,] <- c(AIC(model2),BIC(model2))
criteria <- data.frame(criteria)
rownames(criteria) <- c("model1", "model2")
colnames(criteria) <- c("AIC", "BIC")
criteria
```

These models seem to be very similar (model1 is slightly better).

Let us briefly check the most crucial assumptions (normality of residuals and random effects).

```{r, warning=FALSE, message=FALSE}
normality <- matrix(nrow=2, ncol=2)
normality[1,] <- c(shapiro.test(residuals(model1))$p.value,
                   shapiro.test(ranef(model1, condVar=TRUE)$'student'[,1])$p.value)
normality[2,] <- c(shapiro.test(residuals(model2))$p.value,
                   shapiro.test(ranef(model2, condVar=TRUE)$'student'$'(Intercept)')$p.value)
normality<- data.frame(normality)
rownames(normality) <- c("model1", "model2")
colnames(normality) <- c("residuals", "student (mean)")
normality
```

Non of these assumptions are met. But it is not that surprising as we already concluded that the relation between `lines` and `homework` is not linear. That is why we try the polynomial fit.

## The polynomial model

As earlier we consider two kinds of relation between `homework` and `student`.

```{r, warning=FALSE, message=FALSE}
model3 <- lmer(lines ~ homework + I(homework^2) + (1|student), data=results)
model4 <- lmer(lines ~ homework + I(homework^2) + (homework + I(homework^2)|student), data=results)
```

Now we compare these models.

```{r, warning=FALSE, message=FALSE}
criteria2 <- matrix(nrow=2, ncol=2)
criteria2[1,] <- c(AIC(model3),BIC(model3))
criteria2[2,] <- c(AIC(model4),BIC(model4))
criteria2 <- data.frame(criteria2)
rownames(criteria2) <- c("model3", "model4")
colnames(criteria2) <- c("AIC", "BIC")
criteria2
```

Again model with no relation between `homework` and `student` is slightly better.

Now we check normality assumptions.

```{r, warning=FALSE, message=FALSE}
normality2 <- matrix(nrow=2, ncol=2)
normality2[1,] <- c(shapiro.test(residuals(model3))$p.value,
                   shapiro.test(ranef(model3, condVar=TRUE)$'student'[,1])$p.value)
normality2[2,] <- c(shapiro.test(residuals(model4))$p.value,
                   shapiro.test(ranef(model4, condVar=TRUE)$'student'$'homework')$p.value)
normality2<- data.frame(normality2)
rownames(normality2) <- c("model3", "model4")
colnames(normality2) <- c("residuals", "student")
normality2
```

Again the assumptions are not met.

## No `student` effect?

Maybe there is no `student` effect at all?

```{r, warning=FALSE, message=FALSE}
model5 <- lm(lines ~ homework, data=results)
model6 <- lm(lines ~ homework + I(homework^2), data=results)
model7 <- lm(lines ~ homework + I(homework^2) + I(homework^3), data=results)
anova(model5, model6)
anova(model6, model7)
```

Here we see that quadratic relation between `lines` and `homework` is probably the best one.

```{r, warning=FALSE, message=FALSE}
criteria3 <- matrix(nrow=3, ncol=2)
criteria3[1,] <- c(AIC(model5),BIC(model5))
criteria3[2,] <- c(AIC(model6),BIC(model6))
criteria3[3,] <- c(AIC(model7),BIC(model7))
criteria3 <- data.frame(criteria3)
rownames(criteria3) <- c("linear", "quadratic", "cubic")
colnames(criteria3) <- c("AIC", "BIC")
criteria3
```

BIC criterion convince us in this choice.

```{r, warning=FALSE, message=FALSE}
shapiro.test(rstandard(model5))
shapiro.test(rstandard(model6))
shapiro.test(rstandard(model7))
```

Once again residuals are not normally distributed.

```{r, warning=FALSE, message=FALSE}
par(mfrow=c(3,2))
plot(model6, which=1:6)
```

Probably the biggest problem is that the residuals have a heavy right tail. There are no influential observations.

Maybe *not really hardworking students* are the problem? Let us remove them from the dataset.

```{r, warning=FALSE, message=FALSE}
results2 <- results[as.character(results$student) %in% c("bogdanska","frej","gajewska","kanska","koster","obarki","pompe","pujszo","sitko","wojno"),]
results2$student <- factor(results2$student)
```

Does it change anything?

```{r, warning=FALSE, message=FALSE}
model8 <- lm(lines ~ homework + I(homework^2), data=results2)
shapiro.test(rstandard(model8))
```

Not really.

## Transformation of the data

We try Box-Cox transformation.

```{r, warning=FALSE, message=FALSE}
library(MASS)
tmp <- boxcox(lines ~ homework + I(homework^2), data=results, lambda=seq(-1, 1, by=.005))
(lambda <- tmp$x[which.max(tmp$y)])

results$linesT <- (results$lines^lambda - 1)/lambda
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, fig.height=5, fig.width=7}
ggplot(results, aes(x=homework, y=linesT)) +
  geom_point(size=2) +
  stat_smooth(aes(colour="linear"), method = "lm", se=FALSE, formula = y ~ x) +
  stat_smooth(aes(colour="2nd order"), method = "lm", se=FALSE, formula = y ~ x + I(x^2)) +
  stat_smooth(aes(colour="3rd order"), method = "lm", se=FALSE, formula = y ~ x + I(x^2) + I(x^3)) +
  scale_colour_discrete("") +
  ggtitle("Polynomial fit (transformed data)") + 
  theme(plot.title = element_text(lineheight=.8, face="bold"))
```

Again we suppose there is 2nd order polynomial relation.

```{r, warning=FALSE, message=FALSE}
model9 <- lm(linesT ~ homework + I(homework^2), data=results)
shapiro.test(rstandard(model9))
```

Now the residuals are normally distributed (we cannot reject the null hypotesis).

## Mixed model once again

Now let us include the effect of `student`.

```{r, warning=FALSE, message=FALSE}
model10 <- lmer(linesT ~ homework + I(homework^2) + (1|student), data=results)
shapiro.test(residuals(model10))
shapiro.test(ranef(model10, condVar=TRUE)$'student'[,1])
```

These results suggest that `student` variable should not be included.

## Final model

```{r, warning=FALSE, message=FALSE}
summary(model9)
```

Finally we get the relation

`lines` = [ 1 - 0.27( 0.068717 `homework` - 0.005228 `homework`^2^ ) ] ^-1/0.27^

## Conclusions

* It is recommended to do the graphical analysis in the first places. For example looking at the histogram of `lines` variable could have made us consider the data transformation earlier.
* There is no relation between `lines` and `student`, so there was no need to consider whether I should include myself as a fixed or a random effect.
* There is a non-linear relation between `lines` and `homework`. This implies that simply some homeworks demanded more work than others and it had a very similar impact on all of the students.
