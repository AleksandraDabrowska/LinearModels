---
title: "Untitled"
author: "Emilia Pompe"
date: "Wednesday, January 20, 2016"
output: html_document
---
## Emilia Pompe
## date: "Wednesday, January 20, 2016"

## Introduction

Create a model that explains the number of lines of homework. What is the relation between number of lines of the homework and the id / week of the homework?

Let us take a look at the dataset:
```{r}
load(file="nr.lines.data.RData")
colnames(nr.lines.data) <- c("nr.hw", "student", "nr.lines")
summary(nr.lines.data)
nr.lines.data$nr.lines <- as.numeric(as.character(nr.lines.data$nr.lines))
nr.lines.data$student <- as.factor(nr.lines.data$student)
nr.lines.data$nr.hw <- as.numeric(as.character(nr.lines.data$nr.hw))
```

Let us see how many homeworks were submitted by each student:
```{r}
sort(table(nr.lines.data$student), decreasing=TRUE)
```

It is observable that the number of homeworks submitted by each student varies significantly, hence later on it might be reasonable to delete from the data set observations related e.g to those students who have dropped the course.

## Graphical analysis
I tried to fit a linear function, a quadratic function and a third order polynomial to my dataset.
```{r}
library(ggplot2)
ggplot(nr.lines.data, aes(x=nr.hw, y=nr.lines)) +
  geom_point(size=3) + geom_smooth(method="lm", col="red", se=FALSE, size=1.5) +
  geom_smooth(method="lm", formula=y~x+I(x^2), col="blue", size=1.5, se=FALSE) +
  geom_smooth(method="lm", formula=y~x+I(x^2) + I(x^3), col="green", size=1.5, se=FALSE)
```

Note that linear fitting is not appropriate in this case (the red line is almost horizontal). What is more, increasing the order of polynomial from 2 to 3 did not improve the fitting significantly, as the blue line (second order) and the green line (third order) are very close to each other.

Let us also examine the 'effect of student' graphically. To do it, I chose only those students, who submitted at least 5 homeworks, namely: AK, AS, EP, KK, PiOb, MP, MB, MF.
```{r}
nr.lines.data.part <- subset(nr.lines.data, student %in% c("AK", "AS", "EP", "KK", "PiOb", "MP", "MB", "MF"))
ggplot(nr.lines.data.part, aes(x=student, y=nr.lines)) + geom_boxplot() 
```

It may be observed that some students tend to prepare longer homeworks than others, which indicated that an effect of student should be taken into consideration while modelling. From now on I work on the dataset nr.lines.data.part, because I believe only these students should be taken into consideration in this model as the number of observations for the remaining ones in insufficient.

Let us also take a look at the distribution of the number of lines itself.
```{r, warning=FALSE, message=FALSE}
par(mfrow=c(1,2))
hist(nr.lines.data.part$nr.lines)
hist(log(nr.lines.data.part$nr.lines-10))
```

The second histogram is more similar to the histogram of the Gaussian distribution, that is why I will use log(nr.lines.data - 10). Let us now plot a similar as the first one, but with a reduced data set and logarithm.
```{r}
ggplot(nr.lines.data.part, aes(x=nr.hw, y=log(nr.lines-10))) +
  geom_point(size=3) + geom_smooth(method="lm", col="red", se=FALSE, size=1.5) +
  geom_smooth(method="lm", formula=y~x+I(x^2), col="blue", size=1.5, se=FALSE) +
  geom_smooth(method="lm", formula=y~x+I(x^2) + I(x^3), col="green", size=1.5, se=FALSE)
```

It is hard to decide which curve gives the best fit, but it seems to me that again the quadratic should be considered.

## Linear models
If we want to include an effect of student, it should be treated as a random, not fixed effect, as we are not interested in effects of particular students (they are not going to attend the same course in the future).
Model1 does not include random slope, while model2 includes it.. The results for it are as follows:
```{r, message=FALSE, warning=FALSE}
library(lme4)
model1<- lmer(log(nr.lines-10) ~ nr.hw + I(nr.hw^2) + (1|student), data=nr.lines.data.part, REML=F)
model2<- lmer(log(nr.lines-10) ~ nr.hw + I(nr.hw^2) + (nr.hw |student), data=nr.lines.data.part, REML=F)
anova(model1, model2)
c(AIC(model1), AIC(model2))
c(BIC(model1), BIC(model2))
```

Model1 turned out to be better in all 3 criterias, that is why I will use it as the 'final' one.
```{r, message=FALSE, warning=FALSE}
summary(model1)
```
Both fixed effects are significant. It may be deduced from high absolute values of t-values.

## Assumptions of the final model
Let us test the normality assuptions.
```{r, message=FALSE, warning=FALSE}
shapiro.test(residuals(model1))
shapiro.test(ranef(model1)$student[,1]) 
```
Residuals of the model are normally distributed. For random effects the p-value was at border of significance but I decided to accept this result, as the sample is very small for performing reliable normality tests.

```{r, message=FALSE, warning=FALSE}
AIC(lm(log(nr.lines-10) ~ nr.hw + I(nr.hw^2), data=nr.lines.data.part))
BIC(lm(log(nr.lines-10) ~ nr.hw + I(nr.hw^2), data=nr.lines.data.part))
```

Both criterias indicate  that the model wihout random effects would be worse. Hence, the effect of student is significant.

## Conlusions
The first conclusion is that number of lines is a quadratic function, its maximum was close to the middle of the course. What is more, it was confirmed that the number of lines depends upon individual students, as some of them tend to submit longer homeworks.
